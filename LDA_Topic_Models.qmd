---
title: "Applying LDA Topic Models to Analyze Research Abstracts"
author: Julie Nguyen
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
editor: visual
execute:
  cache: true
---

Welcome to the next phase of my journey preparing for my PhD comprehensive exams! Here, we delve deeper into the world of Natural Language Processing (NLP) to unearth the hidden themes within the 100+ research papers on my reading list. Previously, we've analyzed these papers' abstracts using TF-IDF and weighted log odds. Now, we'll explore topic modeling—a great method to detect patterns and themes across these vast collections of text.

**What is Topic Modeling?**

Imagine entering a bustling cafe where every table buzzes with a different conversation. Topic modeling is somewhat like having a superpower that lets you instantly understand the gist of every conversation in the room. It groups words that often appear together into themes or topics. Each topic is a mixture of words, and each document (or abstract, in our case) is a mixture of topics. This way, instead of poring over every paper, we can quickly grasp the major discussions in a field.

**Here's our exploration roadmap:**

1.  Preparation for Topic Modeling: We begin by transforming our cleaned text data from a tidy format, where each word is isolated in its own row, back to a wide format where each document is represented in a single row. This preparation is crucial for topic modeling.
2.  Creating a Document-Term Matrix (DTM): We convert the wide-format text data into a document-term matrix. This matrix serves as the input for our topic modeling, enabling the detection of textual patterns.
3.  Training Topic Models: Using Latent Dirichlet Allocation (LDA), we train a series of topic models, experimenting with topics ranging from 10 to 130. Our objective is to identify the model that most accurately captures the core themes, assessed by their topic coherence.
4.  Evaluating Models: We evaluate the semantic coherence of each model and use visualizations to determine the most effective model.
5.  Analyzing Topic Model Results: Once we select our model, we delve into the topics it uncovered. We examine the prevalence and coherence of each topic and employ visualizations to show the most dominant topics and their pivotal terms.
6.  Detecting Communities of Topics: Lastly, we explore how topics relate to each other using a dendrogram, which visually represents how topics cluster based on their co-occurrence in documents. This helps us understand which topics commonly intersect, shedding light on their relationships.

Join me as we transform academic text into a structured exploration of academic discourse across 100+ papers!

# Creating input for modeling

First things first, let's load the data we created in the last notebook. This dataset includes hundreds of abstracts from academic papers, with each row representing a single paper's abstract along with its metadata, such as the title, authors, publication year, and the journal it appeared in.

```{r setup}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r}
library(dplyr) # manipulate data
library(tidyr) # tidy data
library(ggplot2) # visualize data
library(kableExtra) # create pretty table
library(textmineR) # perform topic modeling
library(tidytext) # text mining tasks specific to tidy data
library(stringr) # string operations
library(purrr) # functional programming tools
library(furrr) # parallel processing using purrr
library(dendextend) # create and manipulate dendrogram objects
library(ComplexHeatmap) # create complex heatmaps
library(circlize) # circular visualization

# Setup parallel processing to speed up computationally intensive tasks
plan(multisession)
```

```{r}
# Load the dataset created in previous notebook on paper abstract and paper metadata
reading <- readRDS("reading.rds")

# Preview the first 2 entries of the dataset 
reading %>% head(2) %>% kable()
```

Currently, each paper is just a number in our dataset. Let's give them names that resonate more closely with their identity by combining the first author's name and the publication year. It's like assigning a more memorable name tag to each guest at our text analytics party!

```{r}
# Prepare the data by creating a unique identifier for each document using the original author name column
reading %>%
  mutate(id = str_replace(first_author, ", ", "_")) %>% 
  select(-first_author) -> reading
```

With our data neatly labeled, let's dive into text processing. Here’s what we do: - Break down each abstract into its fundamental building blocks—individual words. - Clean up these words by removing common stop words and numbers that don’t add much meaning. - Standardize words to their root forms, turning plurals into singulars. This way, variations like "results" and "result" are treated as one. - Reassemble our abstracts into the wide format (i.e., one document per row).

```{r}
# Preparing the abstracts for topic modeling 
reading_unnest <- reading %>%
  # Selecting only the ID and abstract text columns for analysis
  select(id, abstract) %>% 
  # Tokenizing abstracts into individual words
  unnest_tokens(word, abstract) %>% 
  # Removing common English stop words to focus on relevant terms
  anti_join(get_stopwords()) %>% 
  # Filtering out tokens that are numeric as they likely do not contribute to thematic analysis
  filter(!str_detect(word, "[0-9]+")) %>% 
  # Normalizing plural words to their singular form to consolidate word forms
  mutate(word = case_when(str_detect(word, "[^e|aies$]ies$") ~ str_replace(word, "ies$", "y"),
                          str_detect(word, "[^e|a|oes$]es$") ~ str_replace(word, "es$", "e"),
                          str_detect(word, "[^ss$|us$]s$") ~ str_remove(word, "s$"),
                          TRUE ~ word)) %>% 
  # create id for each word within a reading
  group_by(id) %>% 
  mutate(ind=row_number()) %>% 
  # convert long table to wide table to create a column for each word in each reading 
  pivot_wider(id, 
              names_from = ind,
              values_from = word) %>% 
  # reorder rows based on id 
  arrange(id) %>% 
  # replace NA with empty space
  mutate_all(replace_na, "") %>% 
  # merge one-word columns together
  unite(col = abstract_ns, 
        # leave out id column
        -id, 
        # use a "" as a separator
        sep = " ",
        # remove input column
        remove = T)  -> reading_input

# Display the processed input data for topic modeling
reading_input %>% head(5) %>% kable()
```

Once our abstracts are prepared, we transform them into a document-term matrix, which serves as input for our topic modeling.

```{r, eval=F}
# create document-term matrix as input for topic model
CreateDtm(doc_vec = reading_input$abstract_ns, 
          doc_names = reading_input$id,
          # choose unigrams and bigrams
          ngram_window = c(1, 2)) -> reading_dtm
```

# Training and evaluating topic models

Now we can start training our topic models `r emo::ji("hooray")` `r emo::ji("hooray")`. We experiment with different numbers of topics, from 10 to 130. It's like tuning a radio; we're trying to find the right frequency that brings out the clearest signal or, in our case, the clearest themes.

```{r, eval=F}
# Train multiple LDA models with varying numbers of topics (from 10 to 130)
many_models_lda <- tibble(K = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130)) %>%
  mutate(topic_model = future_map(K, ~ FitLdaModel(dtm = reading_dtm, k = ., iterations = 5000))) %>% 
  arrange(K) %>% 
  # calculate semantic coherence
  mutate(semantic_coherence = map_dbl(topic_model, function(x) mean(x$coherence)))

# Save the trained models to an RDS file
saveRDS(many_models_lda, "many_models_lda.rds")
```

One way to find the "best" model (I use quotation marks because there are no hard and fast rules for choosing a model, it depends on your purpose), but one way is through semantic coherence, a measure how logically connected the words in a topic are. It helps us determine if a topic really makes sense or if it just throws together a bunch of unrelated words. `TextmineR` uses a special method called probabilistic coherence, with the following steps: - `TextmineR` first looks at the probability that if one word (say, "athlete") appears in a document, another related word (say, "sports") also appears. - It then compares this contextual probability to the general probability of finding the second word ("sports") in any document, regardless of the presence of the first word ("athlete"). The intuition here is that if the first word significantly increases the likelihood of the second word's presence more than its general occurrence across all documents, then the two words have a meaningful connection specific to that topic. If "sports" generally appears often across all documents, but its presence is not notably increased by the presence of "athlete," then perhaps their pairing isn't as significant as it initially seemed. - `TextmineR` assesses overall topic coherence by calculating such differences for all pairs of top words in a topic and averaging them. A higher average indicates that words in a topic significantly enhance the presence of each other, reflecting a well-connected, meaningful topic.

The essence of TextmineR's semantic coherence measure lies in determining not just the co-occurrence but the meaningful enhancement of word presence by other words within a topic.

Ok, let's calculate the average topic coherence for each of our models and create a line graph to see how topic coherence changes across a range of topic numbers. This graph helps us see which model does the best job at grouping words into meaningful topics.

```{r, eval=F}
readRDS("many_models_lda.rds") -> many_models_lda
# Visualize the semantic coherence of each model to identify the optimal number of topics
many_models_lda %>% 
  ggplot(aes(K, semantic_coherence)) +
  geom_line(size = 1.5,
            color = "#be1558") +
  labs(x = "K (number of topics)",
       y = "Average Coherence of Topics",
       title = "Semantic coherence by number of topics") +
  theme_minimal() +  # Use a minimal theme as a base

# Remove the series of topic models trained from memory to free up space
rm(many_models_lda)
```

```{r, echo=FALSE}
knitr::include_graphics("sem_coh.png")
```

It seems that 100 topics give us the model with highest semantic coherence. Let's get the results of that model and see what it gives us!

```{r, eval = F}
# Retrieve the model with the highest semantic coherence
many_models_lda %>% 
  slice_max(semantic_coherence) %>% 
  pull(K) -> k

# get the output of the model with higest sematic coherence
model_100 <- many_models_lda %>% 
  filter(K == k) %>% 
  pull(topic_model) %>% 
  .[[1]]

# Save the best performing model to an RDS file
saveRDS(model_100, "model_100.rds")
rm(many_models_lda)
```

# Analyzing topic model results

As a first step to understanding LDA results, let's get a summary table describing the topics LDA discovers. The `SummarizeTopics()` gives us the following metrics about each topic:

-   `label` offers a quick tag or name for the topic, derived from its most defining terms.
-   `prevalence` is a measure of popularity. It tells us how often a topic shows up in our dataset. If our dataset were a dinner party, a high prevalence means a topic is a common guest at many tables; it’s something that captures the interest or concern of many researchers.
-   `coherence` measures how much sense a topic makes. It's about clarity and connection. When a topic has high coherence, it means the words that make up this topic aren't just random words thrown together; they share a meaningful relationship, making the topic understandable and insightful.
-   `top_terms_phi` and `top_terms_gamma` give us a peek into the key words that define each topic. While both relate to terms important to the topic, they come from slightly different angles:
    -   `top_terms_phi` points to the most common words within the topic. These are like the common themes or general knowledge that everyone in this topic area talks about—they’re familiar and frequently used.
    -   `top_terms_gamma`, on the other hand, shows the words most exclusive to the topic—these words are rare or unique in other discussions but common in this one, like the signature of the topic.

```{r}
model_100 <- readRDS("model_100.rds")
# create summary of topics
SummarizeTopics(model_100)  -> model_100_sum

model_100_sum %>% 
  kbl(caption = "Summary of 100 topics", col.names = NULL) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"), fixed_thead = T) %>% 
  add_header_above(c("ID","topic","% of discourse","coherence","central terms \n high pr(term | topic)","specific terms \n high pr(topic | term)")) %>% 
  scroll_box(width = "100%", height = "700px")
```

Next, let's see the 20 topics that are most prevalent in our corpus and create a graph for them.

```{r}
model_100_sum %>% 
  # get the 20 most prevalent topics
  slice_max(prevalence, n = 20) %>% 
  # reorder the topics based on their prevalence
  mutate(topic = reorder(topic, prevalence)) %>% 
  # create a graph with topic on the x axis, prevalence in the y axis, 
  # add their most central terms as label, give each topic a different color
  ggplot(aes(x = topic, 
             y = prevalence, 
             label = top_terms_phi, 
             fill = topic)) +
  # create a bar graph
  geom_col(show.legend = FALSE) +
  # horizontal alignment for text as bottom (0) so that the label will not overlap with bars
  geom_text(hjust = 0,
            # nudge label for y axis variable to create white space between label and bars
            nudge_y = 0.02,
            # change font type and font size of the label
            size = 3,
            family = "IBMPlexSans") +
  # flip x axis and y axis
  coord_flip() +
  # get rid of the space between bars and axis labels on y axis
  scale_y_continuous(expand = c(0,0),
                     # set limit for the y axis so that all the labels will appear
                     limits = c(0, 3.5)) +
  # change to theme tufte which strips graph of border, axis lines, and grids
  ggthemes::theme_tufte(base_family = "IBMPlexSans", ticks = FALSE) +
  # change font size of title and subtile
  theme(plot.title = element_text(size = 16, family="IBMPlexSans-Bold"),
        plot.subtitle = element_text(size = 13)) +
  # change text for axis and title and subtle
  labs(x = NULL, y = "Topic prevalence",
       title = "What are the most prevalent topics in the corpus?",
       subtitle = "With the top 5 most central words that contribute to each topic")
```

Let's also see the most coherent topics in our corpus.

```{r}
model_100_sum %>% 
  slice_max(coherence, n = 20) %>% 
  mutate(topic = reorder(topic, coherence)) %>% 
  ggplot(aes(x = topic, 
             y = coherence, 
             label = top_terms_phi, 
             fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, 
            nudge_y = 0.02,
            size = 3,
            family = "IBMPlexSans") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 2.5)) +
  ggthemes::theme_tufte(base_family = "IBMPlexSans", ticks = FALSE) +
  theme(plot.title = element_text(size = 16, family="IBMPlexSans-Bold"),
        plot.subtitle = element_text(size = 13)) +
  labs(x = NULL, y = "Topic coherence",
       title = "What are the most coherent topics in the corpus?",
       subtitle = "With the top 5 most central words that contribute to each topic")
```

Next, let's create a scatterplot to see the relationship between topic coherence and prevalence.

```{r}
model_100_sum %>% 
  ggplot(aes(coherence, prevalence)) + 
  geom_point(
    color = case_when(model_100_sum$prevalence > median(model_100_sum$prevalence) ~ "#d9a5b3", 
                      TRUE ~ "#1868ae"),
    size = 2) +
  ggrepel::geom_text_repel(aes(label = topic),
                           family = "IBMPlexSans",
                           box.padding   = 0.7,
                           direction = "y") +
  labs(title = "Scatterplot of topic prevalence and coherence",
       subtitle = "With above-median-prevalence topics in pink and below-median-prevalence \n topics in blue") 
```

We can also get the papers that are most associated with a particular topic. To do that, we extract the topic prevalence for each paper, which is stored in the theta matrix in LDA results.

```{r}
# get topic prevalence in each paper
paper_topic <- as.data.frame(model_100$theta)

# create id for each paper using row names
paper_topic$id <- row.names(paper_topic) 

# merge with orginal data
reading <- left_join(reading, paper_topic)
```

```{r}
# write a function to filter out documents highly associated with each topic
get_top_paper <- function(topic) {
  var <- rlang::sym(topic) # when writing a function in which we use string as input for dplyr function such as filter(), need to use sym() and !! in the rlang package for the string variables
  reading %>% 
    # select citation, abstract and topic columns
    select(citation, abstract, !!var) %>% 
    # filter rows when prevalence of a particular topic is higher than 10 perceent
    filter(!!var > .1) %>% 
    # reorder rows based on topic names
    arrange(desc(!!var)) %>% 
    # rename variable
    rename(theta = !!var) -> top_paper
  return(top_paper)
}

# apply the function to each topic 
# select columns that start with "t_" then get column names and turn them into a variable called "topic" in a tibble
tibble(topic = reading %>% 
         select(starts_with("t_")) %>% 
         colnames()) %>%
  # apply function get_top_paper to each row in variable topic to create a variable called "top_paper" in a tibble
  mutate(top_paper = map(topic, get_top_paper)) %>% 
  # unnest list variable top_paper
  unnest(top_paper) %>% 
  # create pretty table using kable
  kbl(caption = "Topics that each paper is highly associated with") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"),
                fixed_thead = T) %>%
  scroll_box(width = "100%", height = "700px")
```

# Detecting communities

## Communities of topics

Next, let's take a look at the relationship between our topics based on how frequent they cooccur in an abstract. One way to do this is to generate a dendrogram of topics. Here, topics closer to each other are more likely to be present in the same abstract than topics further apart form each other.

```{r, fig.height= 25, fig.width=14}
# get topic prevalence in each paper
paper_topic <- as.data.frame(model_100$theta)

# get the data with paper id as rows and topic prevalence as colum 
paper_topic[,1:ncol(paper_topic)-1] %>% 
  # convert this dataframe into a matrix
  as.matrix() %>% 
  # transpose the matrix so that paper id is now the columns and topic prevalence is now the rows
  t() %>% 
  # calculate the Jensen Shannon distance between each topic's vectors of probability that it is present in the abstract of each of 103 papers (i.e., comparing topics based on its presence in the papers)
  philentropy::JSD() %>% 
  # convert to a distance matrix
  as.dist() %>% 
  # conduct hierarchical cluster analysis on the topics
  hclust(method = "ward.D") -> topic_clusters

# rename topic labels 
topic_clusters$labels <- paste(model_100_sum$topic, model_100_sum$label_1)

topic_clusters %>% 
  # convert to dendrogram
  as.dendrogram() %>%
  # set label color 
  set("labels_col", value = c("#ecbdb1", "#f46e7b", "#f23955", "#7fd1c0", "#baf0e4",
                              "#ecbdb1", "#f46e7b", "#f23955", "#7fd1c0", "#baf0e4",
                              "#ecbdb1", "#f46e7b", "#f23955", "#7fd1c0", "#baf0e4",
                              "#ecbdb1", "#f46e7b", "#f23955", "#7fd1c0", "#baf0e4"), k=20) %>% 
  # set label size
  set("labels_cex", 1) %>% 
  # set branches color
  set("branches_k_color", value = c("#ecbdb1", "#f46e7b", "#f23955", "#7fd1c0", "#baf0e4",
                              "#ecbdb1", "#f46e7b", "#f23955", "#7fd1c0", "#baf0e4",
                              "#ecbdb1", "#f46e7b", "#f23955", "#7fd1c0", "#baf0e4",
                              "#ecbdb1", "#f46e7b", "#f23955", "#7fd1c0", "#baf0e4"), k = 20) %>%
  # set branches size
  set("branches_lwd", 1.5) %>% 
  as.ggdend() %>% 
  ggplot(horiz = T) +
  scale_y_reverse(expand = c(0.5, 0))
  
```

## Communities of papers

We can also visualize the relationship among our papers based on how frequent they include the same topics in their abstract. Once again, let’s create a dendrogram, but instead of topics this is a dendrogram of papers. Here, papers closer to each other are more likely to be include the same topics in their abstract than papers further apart form each other.

```{r, fig.height= 25, fig.width=14}
# get the data with paper id as rows and topic prevalence as colum 
paper_topic[,1:ncol(paper_topic)-1] %>% 
  # convert this dataframe into a matrix
  as.matrix() %>% 
  # calculate the Jensen Shannon distance each paper's vector of probability that each of the 86 topics is present in its abstract (i.e., comparing papers based on its topic distribution)
  philentropy::JSD() %>% 
  # convert to a distance matrix
  as.dist() %>% 
  # conduct hierarchical cluster analysis on the topics
  hclust(method = "ward.D") -> paper_clusters

# rename topic labels 
paper_clusters$labels <- reading$id

paper_clusters %>% 
  # convert into dendrogram
  as.dendrogram() %>%
  # set label color 
  set("labels_col", value = c("#058fb2", "#f9ae33", "#47c7eb", "#c8d9cd", "#527e4b",
                              "#058fb2", "#f9ae33", "#47c7eb", "#c8d9cd", "#527e4b",
                              "#058fb2", "#f9ae33", "#47c7eb", "#c8d9cd", "#527e4b",
                              "#058fb2", "#f9ae33", "#47c7eb", "#c8d9cd", "#527e4b"), k=20) %>% 
  # set label size
  set("labels_cex", 1) %>% 
  # set branches color
  set("branches_k_color", value = c("#058fb2", "#f9ae33", "#47c7eb", "#c8d9cd", "#527e4b",
                              "#058fb2", "#f9ae33", "#47c7eb", "#c8d9cd", "#527e4b",
                              "#058fb2", "#f9ae33", "#47c7eb", "#c8d9cd", "#527e4b",
                              "#058fb2", "#f9ae33", "#47c7eb", "#c8d9cd", "#527e4b"), k = 20) %>%
  # set branches size
  set("branches_lwd", 1.5) %>% 
  as.ggdend() %>% 
  ggplot(horiz = T) +
  scale_y_reverse(expand = c(0.5, 0))
```

# Group differences in topic distributions

Now let’s move on the how topic prevalence differ across different groups of papers.

## Differences across fields

To visualize how fields differ in the topics they mentioned, we can create a heatmap that uses shades of color to depict variation in topic prevalence in each field - the darker the shades the more prevalent a topic is.

```{r}
# create id for each paper using row names
paper_topic$id <- row.names(paper_topic) 

#converting the document topic matrix to a "tidy" form
pivot_longer(paper_topic,
             cols=starts_with("t_"),
             names_to="topic",
             values_to="prevalence") %>% 
  # rename topics so that they will be ordered according to topic number
  mutate(topic = case_when(
    str_detect(topic, "_([0-9]$)") ~ sub("_([0-9]$)", "_00\\1", topic),
    str_detect(topic, "_([1-9][0-9]$)") ~ sub("_([1-9][0-9]$)", "_0\\1", topic),
    TRUE ~ topic)) -> paper_topic_l
```

```{r}
paper_topic_l %>%
  # get field column from original data
  left_join(reading %>% select(id, field)) %>%  
  # get average topic prevalence for each field
  group_by(topic, field) %>%
  summarize(mp = mean(prevalence)) %>% 
  # cast back to "wide" format with each row as a field and each column as a topic
  pivot_wider(values_from = mp,
              names_from = topic) -> field_topic


# convert to matrix
field_topic %>% 
  # remove field variable
  select(-field) %>% 
  # transpose the matrix
  t() %>% 
  # convert to matrix type
  as.matrix() -> field_topic_mat

# change row names - topic labels
rownames(field_topic_mat) <- paste(model_100_sum$topic, model_100_sum$label_1)

# change column names - field labels
colnames(field_topic_mat) <- field_topic$field
```

```{r, fig.height= 14, fig.width=14}
# create heatmap
heatmap(field_topic_mat, 
        # normalize column values 
        scale = "column", 
        # set size for row labels
        cexRow = 0.7, 
        # set size for column labels
        cexCol = 1,
        # set the color palette
        col = colorRampPalette(RColorBrewer::brewer.pal(9, "YlOrBr"))(256))
```

This heatmap are quite hard to see because we have many topics. Let’s filter out the 10 most prevalent topics in each field.

```{r}
paper_topic_l %>%
  # get field column from original data
  left_join(reading %>% select(id, field)) %>% 
  # get average topic prevalence for each field
  group_by(topic, field) %>%
  summarize(mp = mean(prevalence)) %>% 
  ungroup() %>% 
  group_by(field) %>% 
  slice_max(mp, n = 10) %>% 
  ungroup() %>% 
  mutate(topic = reorder_within(topic, mp, field)) %>% 
  ggplot(aes(topic, mp, fill = field)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0, 0)) +
  facet_wrap(~field, ncol = 4, scales = "free") +
  labs(x = "Average topic prevalence", y = NULL,
       title = "What are the 10 most prevalent topics in each field?") +                        
  theme(strip.text.x = element_text(size = 11))
```

## Differences across time

Next, let’s look at how topic distribution changes across years.

```{r, fig.height= 14, fig.width=14}
paper_topic_l %>%
  # get field column from original data
  left_join(reading %>% select(id, year)) %>% 
  # get average topic prevalence for each field
  group_by(topic, year) %>%
  summarize(mp = mean(prevalence)) %>% 
  # cast back to "wide" format with each row as a field and each column as a topic
  pivot_wider(values_from = mp,
              names_from = topic) -> year_topic


# convert to matrix
year_topic %>% 
  # remove field variable
  select(-year) %>% 
  # transpose the matrix
  t() %>% 
  # convert to matrix type
  as.matrix() -> year_topic_mat

# change row names - topic labels
rownames(year_topic_mat) <- paste(model_100_sum$topic, model_100_sum$label_1)

# change column names - field labels
colnames(year_topic_mat) <- year_topic$year

# create heatmap
heatmap(year_topic_mat, 
        # normalize column values 
        scale = "column", 
        # set size for row labels
        cexRow = 0.7, 
        # set size for column labels
        cexCol = 1,
        # set the color palette
        col = colorRampPalette(RColorBrewer::brewer.pal(9, "YlOrBr"))(256))
```

Next, we can get the 20 most prevalent topics in each year.

```{r, fig.height= 25, fig.width=14}
paper_topic_l %>%
  # get field column from original data
  left_join(reading %>% select(id, year)) %>% 
  # get average topic prevalence for each field
  group_by(topic, year) %>%
  summarize(mp = mean(prevalence)) %>% 
  ungroup() %>% 
  group_by(year) %>% 
  slice_max(mp, n = 10) %>% 
  ungroup() %>% 
  mutate(topic = reorder_within(topic, mp, year)) %>% 
  ggplot(aes(topic, mp)) +
  geom_col(show.legend = FALSE, fill = "#2BAE66FF") +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0, 0)) +
  facet_wrap(~year, ncol = 3, scales = "free") +
  labs(x = "Average topic prevalence", y = NULL,
       title = "What are the 10 most prevalent topics in each year?") +
  theme(plot.title = element_text(size = 30))
```

We can also examine differences across decades. First let’s create a heatmap.

```{r, fig.height= 14, fig.width=14}
paper_topic_l %>% 
  left_join(reading %>% select(id, decade)) %>% 
  group_by(decade, topic) %>% 
  summarize(prevalence = mean(prevalence)) %>% 
  ungroup() %>% 
  group_by(decade) %>% 
  mutate(prevalence_normalized = prevalence/max(prevalence)) %>% 
  ungroup() %>% 
  select(-prevalence) %>% 
  pivot_wider(values_from = prevalence_normalized,
              names_from = topic) -> decade_topic 

# convert to matrix
decade_topic %>% 
  # remove field variable
  select(-decade) %>% 
  # transpose the matrix
  t() %>% 
  # convert to matrix type
  as.matrix() -> decade_topic_mat

# change row names - topic labels
rownames(decade_topic_mat) <- paste(model_100_sum$topic, model_100_sum$label_1)
# change column names - field labels
colnames(decade_topic_mat) <- decade_topic$decade

# create heatmap
heatmap(decade_topic_mat, 
        # normalize column values 
        scale = "column", 
        # set size for row labels
        cexRow = 0.7, 
        # set size for column labels
        cexCol = 1,
        # set the color palette
        col = colorRampPalette(RColorBrewer::brewer.pal(9, "YlOrBr"))(256))
```

Instead of one heatmap that includes all decades, we can also create a separate heatmap and dendrogram for each decade.

```{r}
heat_map_2 <- function(data, name = "") {
  Heatmap(data, 
          width = unit(0.5, "cm"), 
          col = colorRamp2(c(min(data),max(data)), c("white", "#990011FF")),
          row_names_gp = grid::gpar(fontsize = 7),
          show_column_names = F,
          heatmap_legend_param = list(title = ""),
          column_title = name,
          # show_heatmap_legend = F,
          row_dend_width = unit(9, "cm")) 
          }

heat_map_by_decades <-apply(decade_topic_mat, 2, heat_map_2)
```

What does the topic distribution look like in the 2010s?

```{r, fig.height= 25, fig.width=14}
heat_map_by_decades$`2010s`
```

What about the 2020s?

```{r, fig.height= 25, fig.width=14}
heat_map_by_decades$`2020s`
```

## Differences across two major research streams

Now let’s see how topic distribution differs between the two research streams of my comps.

```{r, fig.height= 25, fig.width=14}
paper_topic_l %>% 
  left_join(reading %>% select(id, research_stream)) %>% 
  group_by(research_stream, topic) %>% 
  summarize(prevalence = mean(prevalence)) %>% 
  ungroup() %>% 
  group_by(research_stream) %>% 
  mutate(prevalence_normalized = prevalence/max(prevalence)) %>% 
  ungroup() %>% 
  select(-prevalence) %>% 
  pivot_wider(values_from = prevalence_normalized,
              names_from = topic) -> theme_topic 

# convert to matrix
theme_topic %>% 
  # remove field variable
  select(-research_stream) %>% 
  # transpose the matrix
  t() %>% 
  # convert to matrix type
  as.matrix() -> theme_topic_mat

# change row names - topic labels
rownames(theme_topic_mat) <- paste(model_100_sum$topic, model_100_sum$label_1)
# change column names - field labels
colnames(theme_topic_mat) <- theme_topic$Topic
```

Let's start with the "Network and gender" stream!

```{r, fig.height= 25, fig.width=14}
heat_map_2(theme_topic_mat[,1], "Network and gender") 
```

```{r, fig.height= 25, fig.width=14}
heat_map_2(theme_topic_mat[,2], "Novelty reception") 
```

Finally let’s also get the most prevalent topics in each theme.

```{r}
paper_topic_l %>%
  # get research stream column from original data
  left_join(reading %>% select(id, research_stream)) %>% 
  # get average topic prevalence for each field
  group_by(topic, research_stream) %>%
  summarize(mp = mean(prevalence)) %>% 
  ungroup() %>% 
  group_by(research_stream) %>% 
  slice_max(mp, n = 20) %>% 
  ungroup() %>% 
  mutate(topic = reorder_within(topic, mp, research_stream)) %>% 
  ggplot(aes(topic, mp, fill = research_stream)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0, 0)) +
  facet_wrap(~research_stream, scales = "free") +
  scale_fill_manual(values = c("#317773", "#E2D1F9")) +
  labs(x = "Average topic prevalence", y = NULL,
       title = "What are the 10 most prevalent topics in each research stream?") +
  theme_minimal()
```
